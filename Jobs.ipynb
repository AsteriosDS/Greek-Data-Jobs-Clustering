{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4403ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from requests_html import HTMLSession\n",
    "import psycopg2 as pg2\n",
    "from geopy import geocoders\n",
    "from futures3 import ThreadPoolExecutor, as_completed\n",
    "import swifter\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from skillNer.general_params import SKILL_DB\n",
    "from skillNer.skill_extractor_class import SkillExtractor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_link = 'https://www.kariera.gr/en/jobs?title=&page=0&limit=20'\n",
    "headers = {\n",
    "    'User-agent':\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'\n",
    "}\n",
    "r = requests.get(main_link, headers=headers)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd9e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = os.environ.get(\"DB_HOST\")\n",
    "port = os.environ.get(\"DB_PORT\")\n",
    "database = os.environ.get(\"DB_NAME\")\n",
    "user = os.environ.get(\"DB_USER\")\n",
    "password = os.environ.get(\"DB_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2992d6",
   "metadata": {},
   "source": [
    "## Main Idea\n",
    "### scrape all main_link + sub_links where sub_links=data%20analyst,data%20scientist,data%20engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061921b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4130ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page = soup.find('li', class_='ant-pagination-next').previous_sibling.text\n",
    "max_pages = int(max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eff259",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0538f8e",
   "metadata": {},
   "source": [
    "### working out how to turn pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd324dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_link_list = []\n",
    "for i in range(0, max_pages + 1):\n",
    "    page_link_list.append(\n",
    "        'https://www.kariera.gr/en/jobs?title=&page={}&limit=20'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8121db",
   "metadata": {},
   "source": [
    "## getting all sub-links from all links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025944c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "session = HTMLSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(session, i):\n",
    "    return [y for y in session.get(i).html.links if '/en/jobs/' in y]\n",
    "\n",
    "\n",
    "link_list = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = []\n",
    "    for i in tqdm(page_link_list):\n",
    "        futures.append(executor.submit(get_links, session, i))\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        link_list.extend(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a934603",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c76032",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "main = 'https://www.kariera.gr/'\n",
    "for i in link_list:\n",
    "    final_list.append(main + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29c323",
   "metadata": {},
   "source": [
    "# building the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(link):\n",
    "    unique_id = link[-5:]\n",
    "    a = ''\n",
    "    r = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    if soup.find('div', class_='h4 UGhAL2CE') is not None:\n",
    "        x = soup.find('div', class_='h4 UGhAL2CE').text\n",
    "        if ('Data Analyst' in x) | ('Data Scientist' in x) | ('Data Engineer'\n",
    "                                                              in x):\n",
    "            try:\n",
    "                job_title = soup.find('div', class_='h4 UGhAL2CE').text\n",
    "            except:\n",
    "                job_title = np.nan\n",
    "            try:\n",
    "                company = soup.find('a', class_='__8_RJM1pE').text\n",
    "            except:\n",
    "                company = np.nan\n",
    "            try:\n",
    "                location = soup.findAll(\n",
    "                    'a', class_='V_oVSppD main-body-text')[0].text\n",
    "            except:\n",
    "                location = np.nan\n",
    "            try:\n",
    "                job_occupation = soup.findAll(\n",
    "                    'a', class_='V_oVSppD main-body-text')[3].text\n",
    "            except:\n",
    "                job_occupation = np.nan\n",
    "            try:\n",
    "                level = soup.findAll('a',\n",
    "                                     class_='V_oVSppD main-body-text')[1].text\n",
    "            except:\n",
    "                level = np.nan\n",
    "            try:\n",
    "                job_type = soup.findAll(\n",
    "                    'a', class_='V_oVSppD main-body-text')[2].text\n",
    "            except:\n",
    "                job_type = np.nan\n",
    "            try:\n",
    "                result = soup.findAll('div', class_='__2DY4wJ3z hi8OBmAZ')\n",
    "                for i in result:\n",
    "                    a = i.get_text(strip=True, separator=' ')\n",
    "                    a = a.replace('\\u202f', '').replace('\\xa0',\n",
    "                                                        '').replace('/', '')\n",
    "            except:\n",
    "                content = np.nan\n",
    "            temp = pd.DataFrame([{\n",
    "                'job_id':\n",
    "                unique_id,\n",
    "                'job_title':\n",
    "                job_title,\n",
    "                'company':\n",
    "                company,\n",
    "                'location':\n",
    "                location,\n",
    "                'job_occupation':\n",
    "                job_occupation,\n",
    "                'level':\n",
    "                level,\n",
    "                'job_type':\n",
    "                job_type,\n",
    "                'content':\n",
    "                a,\n",
    "                'date_scraped':\n",
    "                datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "            }])\n",
    "            return temp\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bdb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = []\n",
    "    for link in tqdm(final_list):\n",
    "        futures.append(executor.submit(process_page, link))\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        temp = future.result()\n",
    "        if temp is not None:\n",
    "            df2 = pd.concat([df2, temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876eeab",
   "metadata": {},
   "source": [
    "## Connect to postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653de352",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = int(port)\n",
    "conn = pg2.connect(host=host,\n",
    "                   port=port,\n",
    "                   user=user,\n",
    "                   password=password,\n",
    "                   dbname=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7654c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(cur):\n",
    "    create_table_command = (\"\"\" \n",
    "    CREATE TABLE IF NOT EXISTS jobs (\n",
    "    job_id int primary key NOT NULL,\n",
    "   job_title text NOT NULL,\n",
    "   company text,\n",
    "   location text,\n",
    "   job_occupation text,\n",
    "   level text,\n",
    "   job_type text,\n",
    "   content text,\n",
    "   date_scraped timestamp NOT NULL\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(create_table_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_job_exists(cur, job_id):\n",
    "    query = (\"\"\" select job_id from jobs where job_id = %s \"\"\")\n",
    "    cur.execute(query, (job_id, ))\n",
    "\n",
    "    return cur.fetchone() is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(cur, df):\n",
    "\n",
    "    l = []\n",
    "    for row in df.iterrows():\n",
    "        if check_if_job_exists(cur, row[1][0]):\n",
    "            pass\n",
    "        else:\n",
    "            l.append(row[1][0])\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b83fe",
   "metadata": {},
   "source": [
    "# Create dataframe with new jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_jobs = update_db(cur, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_jobs = df2[df2['job_id'].apply(lambda x: x in new_jobs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dea587",
   "metadata": {},
   "source": [
    "## Push new jobs to postgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7206dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_table(cur, job_id, job_title, company, location,\n",
    "                      job_occupation, level, job_type, content, date_scraped):\n",
    "    insert_new_jobs = (\n",
    "        \"\"\" INSERT INTO jobs (job_id, job_title, company, location, job_occupation, level,\n",
    "           job_type, content, date_scraped)\n",
    "            VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s);\"\"\")\n",
    "\n",
    "    job_to_insert = (job_id, job_title, company, location, job_occupation,\n",
    "                     level, job_type, content, date_scraped)\n",
    "\n",
    "    cur.execute(insert_new_jobs, job_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b871be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_from_df_to_db(cur, df):\n",
    "    for i, row in df.iterrows():\n",
    "        insert_into_table(cur, row['job_id'], row['job_title'], row['company'],\n",
    "                          row['location'], row['job_occupation'], row['level'],\n",
    "                          row['job_type'], row['content'], row['date_scraped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8434d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_from_df_to_db(cur, new_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f626bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88a8d7",
   "metadata": {},
   "source": [
    "# Fetch jobs table and clean duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pg2.connect(host=host,\n",
    "                   port=port,\n",
    "                   user=user,\n",
    "                   password=password,\n",
    "                   dbname=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn:\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT * FROM jobs\")\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "df = pd.DataFrame(rows).rename(\n",
    "    columns={\n",
    "        0: 'job_id',\n",
    "        1: 'job_title',\n",
    "        2: 'company',\n",
    "        3: 'location',\n",
    "        4: 'job_occupation',\n",
    "        5: 'level',\n",
    "        6: 'job_type',\n",
    "        7: 'content',\n",
    "        8: 'date_scraped'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bf05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1cfb7",
   "metadata": {},
   "source": [
    "## Dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and normalize the input data\n",
    "def fingerprint(row):\n",
    "    title = row['job_title'].lower().strip()\n",
    "    company = row['company'].lower().strip()\n",
    "    location = row['location'].lower().strip()\n",
    "    return title + '-' + company + '-' + location\n",
    "\n",
    "\n",
    "# create a new column 'fingerprint' and store the fingerprint values\n",
    "df['fingerprint'] = df.apply(fingerprint, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc936a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates('fingerprint', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop('fingerprint', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2852e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"DELETE FROM jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_from_df_to_db(cur, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34784bf5",
   "metadata": {},
   "source": [
    "# Fetch non duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47640bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM jobs\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "\n",
    "df = pd.DataFrame(rows).rename(\n",
    "    columns={\n",
    "        0: 'job_id',\n",
    "        1: 'job_title',\n",
    "        2: 'company',\n",
    "        3: 'location',\n",
    "        4: 'job_occupation',\n",
    "        5: 'level',\n",
    "        6: 'job_type',\n",
    "        7: 'content',\n",
    "        8: 'date_scraped'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381059b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7700c",
   "metadata": {},
   "source": [
    "## Cleaning company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861753df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company'] = df['company'].apply(lambda x: x.replace('\\t', '').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a94e6",
   "metadata": {},
   "source": [
    "## Categorizing jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ecdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categories'] = df['job_title'].apply(\n",
    "    lambda x: 'Analytics'\n",
    "    if 'Analyst' in x else ('Engineering' if 'Engineer' in x else 'Science'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1200c1",
   "metadata": {},
   "source": [
    "## Cleaning job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd75d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing spaces\n",
    "df['job_title'] = df['job_title'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5496cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing anything inside paretheses (inclusive)\n",
    "df['job_title'] = df['job_title'].apply(lambda x : re.sub(r'\\s*\\([^()]*\\)', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'].replace('Athina','Athens',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing locations from job titles\n",
    "# it assumes that always all locations will be preceded by ' - '\n",
    "def clean_city_from_job(job, city):\n",
    "    titles = []\n",
    "    for i, j in zip(job, city):\n",
    "        if j in i:\n",
    "            title = i[:-(3 + len(j))]\n",
    "            titles.append(title)\n",
    "        else:\n",
    "            title = i\n",
    "            titles.append(title)\n",
    "    return pd.DataFrame(titles).rename(columns={0: 'job_title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_city_from_job(df['job_title'], df['location']).merge(\n",
    "    df, left_index=True,\n",
    "    right_index=True).drop('job_title_y',\n",
    "                           axis=1).rename(columns={'job_title_x': 'job_title'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1e3d3",
   "metadata": {},
   "source": [
    "## Cleaning content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72729c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(\n",
    "    lambda x: x.replace('Description', '').replace('Company', '').replace(\n",
    "        'ΠΕΡΙΓΡΑΦΗ ΘΕΣΗΣ ΕΡΓΑΣΙΑΣ', '').replace('Job', '').replace(\n",
    "            '’s', '').replace('Purpose', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(\n",
    "    lambda x: re.sub(r'Requisition ID: \\d+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7bd89",
   "metadata": {},
   "source": [
    "## Geolocate locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfcb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gn = geocoders.GeoNames('asterios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['geo'] = df['location'].swifter.apply(lambda x: gn.geocode(x)).apply(\n",
    "    lambda loc: tuple(loc.point) if loc else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['latitude', 'longitude', 'altitude']] = pd.DataFrame(df['geo'].tolist(),\n",
    "                                                         index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce115f",
   "metadata": {},
   "source": [
    "### At this point we are done with the cleaning next up is droping unwanted columns and exporting  the csv for the main app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['job_id', 'job_occupation', 'date_scraped', 'geo', 'altitude'],\n",
    "        axis=1,\n",
    "        inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('job_board.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bb392",
   "metadata": {},
   "source": [
    "## Model Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743cccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only job titles, company and content\n",
    "df_c = df[['job_title', 'company', 'content']].copy()\n",
    "\n",
    "# drop duplicates that evaded our previous fingerprint\n",
    "df_c.drop_duplicates(['job_title', 'company'], keep='first', inplace=True)\n",
    "\n",
    "## Clean content\n",
    "\n",
    "# remove job titles from content\n",
    "jobs = [i for i in df_c['job_title'].unique()] + ['Junior', 'Mid', 'Senior']\n",
    "\n",
    "for i in jobs:\n",
    "    df_c['content'] = df_c['content'].apply(lambda x: x.replace(i, ''))\n",
    "\n",
    "# further cleaning\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove hashtags up to the leading whitespace\n",
    "    text = re.sub(r'#\\S+\\s?', '', text)\n",
    "\n",
    "    # Remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove links with https and www\n",
    "    text = re.sub(r'(https?://\\S+)|(www\\.\\S+)', '', text)\n",
    "\n",
    "    # Remove special character ●\n",
    "    text = re.sub(r'●', '', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "df_c['content'] = df_c['content'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235029c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Skill extraction (NER) with skillNer database and spacy pretrained model\n",
    "\n",
    "! python -m spacy download en_core_web_lg\n",
    "\n",
    "# init params of skill extractor\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def skill_extract(x):\n",
    "    full = []\n",
    "    ngrams = []\n",
    "\n",
    "    try:\n",
    "        # init skill extractor\n",
    "        skill_extractor = SkillExtractor(nlp, SKILL_DB, PhraseMatcher)\n",
    "        # extract skills from job_description\n",
    "        job_description = x\n",
    "        annotations = skill_extractor.annotate(job_description)\n",
    "        full = pd.DataFrame(\n",
    "            annotations['results']['full_matches']).drop_duplicates(\n",
    "                'skill_id')['doc_node_value'].tolist()\n",
    "#         ngrams = pd.DataFrame(\n",
    "#             annotations['results']['ngram_scored']).drop_duplicates(\n",
    "#                 'skill_id').sort_values(\n",
    "#                     'score', ascending=False)['doc_node_value'].tolist()\n",
    "    except:\n",
    "        np.nan\n",
    "\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the skills\n",
    "skills = []\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = []\n",
    "    for i in df_c['content']:\n",
    "        futures.append(executor.submit(skill_extract, i))\n",
    "    for future in as_completed(futures):\n",
    "        skills.extend([future.result()])\n",
    "\n",
    "df_c['skills'] = skills\n",
    "\n",
    "df_c['check'] = df_c['skills'].apply(lambda x: np.nan if len(x) == 0 else 0)\n",
    "\n",
    "df_c.dropna(subset='check', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35859ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up for word2vec\n",
    "sentences = df_c['job_title'].apply(lambda x: [x]) + df_c['skills']\n",
    "\n",
    "model = Word2Vec(sentences, window=10, min_count=1, workers=4)\n",
    "\n",
    "job_titles = [i for i in df_c['job_title'].unique()]\n",
    "\n",
    "d = {}\n",
    "for i in job_titles:\n",
    "    try:\n",
    "        d.update({str(i): model.wv[i]})\n",
    "    except:\n",
    "        d.update({str(i): np.nan})\n",
    "\n",
    "final = pd.DataFrame(d).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering with KMeans\n",
    "avg = {}\n",
    "K = range(2, 20)\n",
    "\n",
    "for num_clusters in K:\n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_init=50, random_state=42)\n",
    "    kmeans.fit(final)\n",
    "    score = silhouette_score(final, kmeans.labels_)\n",
    "    avg[num_clusters] = score\n",
    "\n",
    "max_score = max(avg.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7dc49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster with the max avg silhouette score\n",
    "best_cluster_num = max(avg, key=avg.get)\n",
    "\n",
    "kmeans = KMeans(n_clusters=best_cluster_num)\n",
    "kmeans.fit(final.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clusters\n",
    "final['cluster'] = kmeans.labels_\n",
    "\n",
    "# join clusters with original df\n",
    "skill_df = final['cluster'].reset_index().merge(\n",
    "    df_c,\n",
    "    right_on=df_c['job_title'],\n",
    "    left_on=final['cluster'].reset_index()['index'])[[\n",
    "        'job_title', 'skills', 'cluster'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform dimensionality reduction to 3D using t-SNE\n",
    "tsne = TSNE(n_components=3, random_state=42, perplexity=final.shape[0] - 2)\n",
    "embeddings_3d = tsne.fit_transform(final.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spit out necessary items for streamlit\n",
    "df_c.to_csv('df_c.csv', index=False)\n",
    "pickle.dump(avg, open('avg.pkl','wb'))\n",
    "pickle.dump(kmeans, open('model.pkl','wb'))\n",
    "pd.DataFrame(embeddings_3d).to_csv('embeddings_3d.csv', index=False)\n",
    "skill_df.to_csv('skill_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
